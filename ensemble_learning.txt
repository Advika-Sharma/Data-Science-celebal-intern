üß† **Ensemble Learning - Summary Notes**

**Definition:**
Ensemble learning is a machine learning technique where **multiple models (weak learners)** are combined to form a **stronger, more accurate model**.

**Analogy:** Like consulting multiple people for advice‚Äîindividually may not be perfect, but combined give better results.

---

### üî∞ **Types of Ensemble Methods**

| Type         | Description                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------ |
| **Bagging**  | Trains models independently on **random subsets** (bootstrapped) and aggregates predictions. Reduces **variance**. |
| **Boosting** | Trains models **sequentially**, each correcting the previous model‚Äôs errors. Reduces **bias**.                     |
| **Stacking** | Combines predictions of multiple models using a **meta-model** to improve performance.                             |

---

## 1Ô∏è‚É£ **Bagging (Bootstrap Aggregating)**

**Steps:**

1. **Bootstrap Sampling:** Create N random subsets from training data (with replacement).
2. **Train Base Models:** Use weak learners (e.g., decision trees) trained **in parallel** on subsets.
3. **Aggregate Predictions:**

   * **Classification:** Majority voting
   * **Regression:** Averaging
4. **OOB Evaluation:** Use out-of-bag samples to evaluate model without CV.
5. **Final Prediction:** Aggregate all model predictions.

**Python Pseudocode:**

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data = load_iris()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

base_classifier = DecisionTreeClassifier()
bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
bagging_classifier.fit(X_train, y_train)

y_pred = bagging_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)  # Output: 1.0
```

---

## 2Ô∏è‚É£ **Boosting**

**Concept:** Sequentially train models. Each focuses on the **mistakes** of the previous one.

**Steps:**

1. **Initialize Weights:** Equal weights to all data points.
2. **Train Weak Learner:** Use base model (e.g., decision stump).
3. **Focus on Errors:** Misclassified points get higher weights.
4. **Combine Models:** Weighted majority vote (classification) or sum (regression).

### ‚úÖ **Example: AdaBoost**

**Python Pseudocode:**

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data = load_iris()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

base_classifier = DecisionTreeClassifier(max_depth=1)
adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)
adaboost_classifier.fit(X_train, y_train)

y_pred = adaboost_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)  # Output: 1.0
```

---

## üéØ **Benefits of Ensemble Learning**

* ‚úÖ **Reduces Overfitting**
* ‚úÖ **Better Generalization**
* ‚úÖ **Higher Accuracy**
* ‚úÖ **Noise Robustness**
* ‚úÖ **Flexible with multiple base models**
* ‚úÖ **Balances Bias-Variance Tradeoff**

---

## üß© **Popular Ensemble Techniques**

| Technique                   | Category | Description                                                |
| --------------------------- | -------- | ---------------------------------------------------------- |
| **Random Forest**           | Bagging  | Ensemble of decision trees using bootstrapped data.        |
| **Random Subspace Method**  | Bagging  | Trains models on random subsets of features.               |
| **Gradient Boosting (GBM)** | Boosting | Sequential trees minimizing loss function.                 |
| **XGBoost**                 | Boosting | Optimized GBM with regularization and pruning.             |
| **AdaBoost**                | Boosting | Focuses on hard examples, combines weak classifiers.       |
| **CatBoost**                | Boosting | Handles categorical features natively with regularization. |

---

## üìå Summary Table ‚Äì Ensemble Comparison

| Feature            | Bagging         | Boosting          | Stacking                         |
| ------------------ | --------------- | ----------------- | -------------------------------- |
| Model Training     | Parallel        | Sequential        | Parallel (base), then meta-model |
| Goal               | Reduce Variance | Reduce Bias       | Improve final prediction         |
| Example Algorithms | Random Forest   | AdaBoost, XGBoost | Blend of ML models               |
| Base Learners      | Same or varied  | Typically same    | Can be varied                    |
