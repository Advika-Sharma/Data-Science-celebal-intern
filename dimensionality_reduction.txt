 ðŸŒŸ **Dimensionality Reduction**

Process of reducing the number of features/variables to:

* Reduce model complexity
* Minimize overfitting
* Improve accuracy and training efficiency

---

### ðŸ” **1. Feature Selection**

Selecting a **subset of relevant features** without changing them.

#### ðŸ“Œ Why?

* Avoids noise/irrelevant data
* Improves accuracy
* Reduces training time

#### ðŸ“Š **Types of Feature Selection Models**

* **Supervised** (uses output label)
* **Unsupervised** (doesn't use output label)

---

#### ðŸ“ Supervised Feature Selection Methods

##### a. **Wrapper Methods**

Evaluate subsets of features based on model performance (greedy search).

* **Forward Selection** â€“ Start empty, add features iteratively
* **Backward Elimination** â€“ Start full, remove least important iteratively
* **Bi-directional Elimination** â€“ Combines both
* **Exhaustive Search** â€“ Tries all combinations (computationally expensive)
* **Recursive Feature Elimination (RFE)** â€“ Recursively removes least important features using model weights

> âœ… High accuracy but **computationally expensive**

---

##### b. **Filter Methods**

Use **statistical tests** to rank and select features **before training**.

* **Information Gain** â€“ Reduction in entropy
* **Chi-Square Test** â€“ For categorical features
* **Fisherâ€™s Score** â€“ Supervised technique based on class separation
* **Missing Value Ratio** â€“ Drop features with missing values above threshold
* **Correlation Coefficient** â€“ Select features highly correlated with target
* **Variance Threshold** â€“ Remove low-variance features
* **Mean Absolute Difference (MAD)** â€“ Similar to variance but without squaring
* **Dispersion Ratio (AM/GM)** â€“ Higher ratio = better feature

> âœ… Fast & simple, âŒ ignores model interaction

---

##### c. **Embedded Methods**

Feature selection during model training. Combines strengths of filter & wrapper methods.

* **Regularization**:

  * **L1 (Lasso)** â€“ Shrinks irrelevant feature coefficients to 0
  * **Elastic Net** â€“ Combines L1 & L2
* **Random Forest Importance** â€“ Tree-based ranking based on impurity reduction

> âœ… Efficient & more accurate than filter, less expensive than wrapper

---

### ðŸ§ª **2. Feature Extraction**

Transforms existing features into new ones (original features may be discarded).

#### ðŸ“Œ Goal:

* Reduce dimensionality
* Retain essential information
* Enhance model performance & visualization

#### ðŸ“Š Techniques:

* **PCA (Principal Component Analysis)**
  Linear technique using eigenvectors; maximizes variance
* **LDA (Linear Discriminant Analysis)**
  Supervised; maximizes **class separability**
* **KPCA (Kernel PCA)**
  Handles **non-linear** relationships via kernel trick

---

### âœ… **Choosing Between Techniques**

* Use **Feature Selection** when:

  * You want interpretable models
  * The original features have meaning

* Use **Feature Extraction** when:

  * Reducing redundancy is more important
  * Visualization or efficiency is needed
